\chapter{特徴点マッチングに基づく自己位置推定}

\section{特徴点検出およびマッチングの概要}

本研究では、自己位置推定を行うために、前節までに生成した簡易モデルのテクスチャ画像と入力画像との対応付けを行う。
具体的には、両画像から特徴点を検出し、それらの対応関係を構築することで、入力画像上の2次元特徴点と、3次元モデルに対応付けられたテクスチャ上の点との対応を得る。
これに基づき、カメラの位置および姿勢を推定する。
なお、本研究では壁面等の側面テクスチャのみを対象とし、床面は除外する。
これは、床面では同一のテクスチャパターンが連続して用いられることが多く、特徴点の識別による対応付けが困難であるためである。

特徴点検出およびマッチング手法は、大きく従来手法と学習ベース手法の二つに分類される。
従来手法は、画像の輝度勾配や局所構造に基づいて特徴点および記述子を設計する手法であり、計算過程が明確であるという利点を持つ。
一方で、撮影条件の変化や環境の差異に対するロバスト性には限界がある。
これに対して、学習ベース手法は、深層学習を用いて特徴点検出やマッチングの過程をデータから学習する手法である。
画像全体の文脈情報を考慮した対応付けが可能であり、テクスチャの少ない環境においても比較的安定した対応が得られる利点を持つ。
一方で、従来手法と比較して計算コストが増大する傾向にある。本研究では、これら双方のアプローチについて検討を行う。

\subsection{共通の前処理}

本研究では、従来手法と学習ベース手法のいずれにおいても、特徴点検出に先立ち以下の共通した前処理を適用する。

まず、入力画像および簡易モデルのテクスチャ画像をグレースケール化する。

次に、高解像度画像の処理に対応するため、タイリング処理を導入する。
画像サイズが大きい場合、一度に処理を行うとメモリ制約を受ける場合や、微細な特徴が見逃される場合がある。
そのため、画像を複数のタイル領域に分割し、各タイルに対して個別に特徴点検出を行った後、それらの検出結果を統合することで、画像全体を網羅する特徴点集合を抽出する。

また、処理の高速化を図るため、静的なデータであるテクスチャ画像については事前計算を行う。
自己位置推定の実行時に都度計算するのではなく、事前に特徴点検出および記述子の算出を済ませて保存しておくことで、マッチング時の計算負荷を大幅に低減し、システム全体の処理速度を向上させている。


\section{従来手法による特徴点マッチング}

\subsection{特徴点検出}

本研究では、代表的な局所特徴点検出手法として SIFT および AKAZE を用いる。
SIFT は、ガウシアン平滑化により構築されるスケール空間上で極値点を検出し、スケールおよび回転に対して不変な特徴点を得る手法である\cite{Lowe2004}。
各特徴点に対しては、周囲の勾配分布に基づく特徴量が計算され、高い識別性能を持つ。
AKAZE は、非線形スケール空間に基づいて特徴点を検出する手法であり、Fast Explicit Diffusion を用いることで高速な処理を可能としている\cite{Alcantarilla2013}。
記述子にはバイナリ表現が用いられ、後段のマッチング処理を効率的に行うことができる。

\subsection{特徴点マッチング} 

従来手法では、最近傍探索に基づくマッチング手法を採用する。
記述子間の距離を計算し、最も距離の近い特徴点同士を対応点として選択することで、初期的な対応点集合を得る。
SIFT のような実数値記述子に対しては KD-tree を用いた探索を行い、AKAZE のようなバイナリ記述子に対しては LSH (Locality Sensitive Hashing) を用いた探索を行うことで、記述子の特性に応じた最近傍探索を実現している。

\subsection{マッチングの精度向上}

最近傍探索によって得られる対応点集合には、特徴量の類似度のみでは除去できない誤対応が含まれる可能性がある。
そこで本研究では、複数の手法を組み合わせることで、マッチング精度の向上を図る。
まず、誤対応の除去手法として交差検証（Cross Check）を適用する。
画像間の双方向マッチングを行い、互いに第一近傍となるペアのみを採用することで、整合性の高い対応点を抽出する。
その後、RANSAC を用いて幾何的整合性に基づく外れ値除去を行う。
対応点集合から射影変換モデルを推定し、モデルに適合しない対応点を除外することで、幾何的に整合した対応点集合を得る。
これらの処理により、自己位置推定に用いる対応点の精度を向上させ、後段の位置および姿勢推定における安定性の確保を図る。


\section{学習ベース手法による特徴点マッチング}

\subsection{特徴点検出}

学習ベース手法における特徴点検出手法として SuperPoint を用いる。
SuperPoint は、画像中の特徴点（Interest Point）とそれらに対応する記述子（Descriptor）を同時に推定する特徴点検出手法である。
SuperPoint のアーキテクチャを図\ref{three:one}に示す。
画像全体を入力とする畳み込み型ネットワークとして構成されており、共有されたエンコーダによって特徴を抽出した後、特徴点検出用と記述子生成用の2つのデコーダに分岐することで、単一のフォワードパスで両者を出力する。
この構成により、検出と記述を個別に行う手法と異なり、両タスク間で計算および特徴表現を効率的に共有することが可能となっている\cite{DeTone2018SuperPoint}。
従来手法では、輝度勾配や局所構造といった人手設計された指標に基づいて特徴点検出が行われるのに対し、大量の画像データを用いた学習による安定した特徴点検出が可能である。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/3/superpoint_architecture.png}
    \caption{SuperPointのアーキテクチャ. 出典：DeTone et al.(2018)\cite{DeTone2018SuperPoint} Fig.~1}
    \label{three:one}
  \end{center}
\end{figure}

\subsection{特徴点マッチング} 

学習ベース手法における特徴点マッチング手法として SuperGlue を用いる。
SuperGlue は、注意機構付きグラフニューラルネットワークと最適マッチング層の2つから構成される特徴点マッチング手法である。
SuperGlue のアーキテクチャを図\ref{three:two}に示す。
注意機構付きグラフニューラルネットワークでは、キーポイントの位置と記述子を統合した特徴表現を自己注意および相互注意を交互に用いて段階的に更新する。
最適マッチング層では、キーポイント間のスコア行列に対し、いずれのキーポイントとも対応しない点を吸収するための仮想ノードである dustbin を追加した上で、Sinkhorn アルゴリズムにより最適な部分対応を推定する\cite{Sarlin2020SuperGlue}。
従来手法では、各特徴点を独立に対応付ける最近傍探索が行われるのに対し、特徴点集合全体の文脈情報を考慮した対応付けが可能である。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.9\textwidth]{figures/3/superglue_architecture.png}
    \caption{SuperGlueのアーキテクチャ. 出典：Sarlin et al.(2020)\cite{Sarlin2020SuperGlue} Fig.~3}
    \label{three:two}
  \end{center}
\end{figure}

\subsection{マッチングの精度向上}

SuperGlue により得られた対応点集合には、学習に基づく推定結果であるため、局所的に信頼度の低い対応点が含まれる可能性がある。
まず、従来手法における交差検証に相当する処理として、SuperGlue の matching score に基づく対応点の選別を行う。
matching score は，各特徴点対が正しく対応している可能性の高さを表す指標であり、スコアが所定の閾値以上となる対応点のみを採用することで、整合性の高い対応点を抽出する。
続いて、従来手法と同様に RANSAC を用いて外れ値除去を行う。
これにより、学習ベース手法においても従来手法と同等の基準で誤対応を抑制し、自己位置推定に用いる対応点の精度を向上させる。


\section{テクスチャ画像座標から世界座標への変換}

簡易モデルのテクスチャ画像と入力画像の対応点は、いずれも画像平面上の2次元座標として表現されている。
自己位置推定を行うためには、テクスチャ画像の2次元特徴点を世界座標系における3次元座標へ変換する必要がある。
本研究では、テクスチャに割り当てられた画像座標系と、対応する四角形メッシュの四隅の世界座標を用いて、画像座標系から世界座標系への変換を行う。
以下に、2次元座標を3次元座標へ変換する手順を示す。

\subsection{2次元座標から3次元座標への変換}

本節では、テクスチャ画像上で検出された特徴点を、世界座標系の3次元座標へ逆投影する手法について述べる。
まず、特徴点を取得したテクスチャに割り当てられた四角形メッシュを定義する。
世界座標系において、当該メッシュの4頂点を左上から順に時計回りで $\bm{P}_0, \bm{P}_1, \bm{P}_2, \bm{P}_3$ とする。
このとき、メッシュ平面を張る2つの基底ベクトル $\bm{B}_1, \bm{B}_2$ は、始点 $\bm{P}_0$ を基準として次式で表される。

\begin{equation}
  \bm{B}_1 = \bm{P}_1 - \bm{P}_0, \quad
  \bm{B}_2 = \bm{P}_3 - \bm{P}_0
\end{equation}

次に、第\ref{uv}節で定義したUV座標系における、上記頂点との対応関係を用いる。
3次元メッシュの各頂点 $\bm{P}_i$ には、テクスチャ画像上の対応位置を示すUV座標 $\bm{t}_i = (u_i, v_i)$ ($i=0, \dots, 3$) が定義されている。
この対応関係に基づき、当該メッシュ領域がUV空間上で占める幅 $w_{uv}$ および高さ $h_{uv}$ を次式で定義する。

\begin{equation}
  w_{uv} = u_1 - u_0, \quad
  h_{uv} = v_3 - v_0
\end{equation}

ここで、解像度 $(W, H)$ のテクスチャ画像上において検出された特徴点のピクセル座標を $(x, y)$ とする。
この特徴点がメッシュ内のどの位置にあるかを特定するため、正規化パラメータ $\alpha, \beta \in [0, 1]$ へと変換する。
具体的には、ピクセル座標をUV座標 $(x/W, y/H)$ へと正規化した上で、メッシュ始点 $(u_0, v_0)$ からの相対位置として次式により算出する。

\begin{equation}
  \alpha = \frac{x/W - u_0}{w_{uv}}, \quad
  \beta  = \frac{y/H - v_0}{h_{uv}}
\end{equation}

最後に、得られたパラメータ $\alpha, \beta$ を、世界座標系における基底ベクトルの線形結合として用いることで、
特徴点に対応する3次元座標 $\bm{P}$ を算出する。

\begin{equation}
  \bm{P} = \bm{P}_0 + \alpha \bm{B}_1 + \beta \bm{B}_2
\end{equation}


\section{自己位置推定}

前節で取得した入力画像上の2次元特徴点と、それに対応するテクスチャ上の3次元特徴点の対応関係を用いて、自己位置推定を行う。

本研究では、松下らによって提案された、直交射影誤差に基づく PnPL 問題に対する大域最適解の計算手法を用いる\cite{matsushita2024}。
一般的な反復的最適化手法では、初期値依存により局所最適解に陥る可能性があり、複数の解が存在する場合にそれらを網羅的に求めることが困難である。
また、対応点の配置によっては収束までの反復回数が増加し、計算時間が長くなるため、リアルタイム処理への適用が難しい。
これに対し本手法は、Cayley変換を用いた回転行列の表現により制約条件を除去し、グレブナー基底を用いて解を計算することで、
非反復的に大域最適解を求めることが可能である。これにより、計算時間の削減と、数学的にあり得る複数の解候補の同時導出を実現する。

本研究では、点特徴の対応のみを入力とする自己位置推定を行う。
入力は、画像上の2次元特徴点座標に焦点距離情報を加えたベクトル $(x, y, f)$ と、それに対応する世界座標系における3次元特徴点 $(X, Y, Z)$ である。
推定処理の結果として、虚数解を除いたすべての解候補に対し、目的関数値 $J$、回転行列 $\bm{R}$、および並進ベクトル $\bm{t}$ が出力される。

得られた複数の解候補の中から、本研究では幾何学的制約および推定誤差評価に基づき、以下の手順で最終的な自己位置を決定する。

\begin{enumerate}
  \item \textbf{天地反転解の除外} \\
        大域最適解法により得られた解候補には、数学的には正しいが物理的にあり得ない「天地が反転した解」が含まれる場合がある。
        本研究のカメラ座標系では$Y$軸が画像下方向を向いており、世界座標系では$Z$軸が鉛直上向きに定義されている。
        カメラが正立している場合、世界座標系の$Z$軸（上方向）は、カメラ座標系においては$Y$軸の負の方向の成分を持つはずである。
        
        回転行列 $\bm{R}$ の要素 $R_{23}$ は、世界座標系の$Z$軸ベクトル $(0,0,1)^\top$ をカメラ座標系へ変換した際の$Y$成分に相当する。
        したがって、以下の条件を満たす解のみを採用する。
        \begin{equation}
          R_{23} < 0
        \end{equation}

  \item \textbf{カメラ中心位置の算出} \\
        残った各解候補について，回転行列 $\bm{R}$ および並進ベクトル $\bm{t}$ から，世界座標系におけるカメラ中心位置 $\bm{C}$ を次式により算出する。
        \begin{equation}
          \bm{C} = -\bm{R}^{\top} \bm{t}
        \end{equation}

  \item \textbf{位置誤差による評価} \\
        検証のために現在位置の真値 $\bm{C}_{\mathrm{gt}}$ が既知である場合，推定されたカメラ中心位置 $\bm{C}$ とのユークリッド距離を，位置誤差 $e_{\mathrm{pos}}$ として次式で定義する。
        \begin{equation}
          e_{\mathrm{pos}} = \| \bm{C} - \bm{C}_{\mathrm{gt}} \|
        \end{equation}

  \item \textbf{視線方向ベクトルの算出} \\
        現在の視線方向の真値 $\bm{f}_{\mathrm{gt}}$ が既知である場合，まず各解候補について世界座標系における視線ベクトル $\bm{f}_{\mathrm{world}}$ を次式で定義する。
        これはカメラ座標系における光軸ベクトル $(0,0,1)^\top$ を世界座標系へ逆変換したものである。
        \begin{equation}
          \bm{f}_{\mathrm{world}} = \bm{R}^{\top} (0,0,1)^{\top}
        \end{equation}

  \item \textbf{姿勢誤差による評価と最適解の選択} \\
        推定された視線方向 $\bm{f}_{\mathrm{world}}$ と真値 $\bm{f}_{\mathrm{gt}}$ とのなす角を，姿勢誤差 $e_{\mathrm{rot}}$ として算出する。
        \begin{equation}
          e_{\mathrm{rot}} =
          \arccos
          \left(
            \frac{
              \bm{f}_{\mathrm{world}}^{\top} \bm{f}_{\mathrm{gt}}
            }{
              \| \bm{f}_{\mathrm{world}} \| \, \| \bm{f}_{\mathrm{gt}} \|
            }
          \right)
        \end{equation}
        
        最終的に、位置誤差 $e_{\mathrm{pos}}$ および姿勢誤差 $e_{\mathrm{rot}}$ が所定の閾値以下となる解候補のうち、姿勢誤差 $e_{\mathrm{rot}}$ が最小となるものを最終的な自己位置推定結果として採用する。
        これは、姿勢誤差が大きい解は特徴点マッチングの局所的な誤りを含んでいる可能性が高いためである。
\end{enumerate}