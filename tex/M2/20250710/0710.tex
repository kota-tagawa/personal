\documentclass[]{jarticle}          % 一段組
%\documentclass[twocolumn]{jarticle} % 二段組

\textwidth 180mm
\textheight 255mm
\oddsidemargin -12mm
\topmargin -15mm
\columnsep 10mm

%\vspace{0.5cm} % 一段組の場合はコメントアウトした方が体裁がよいx
%] % 一段組の場合はコメントアウトする

\usepackage{styles/labheadings}
\usepackage[dvipdfmx]{graphicx,color}
\usepackage{amsmath,amssymb}
\usepackage{url}
% 追加
\usepackage[hang,small,bf]{caption}
\usepackage[subrefformat=parens]{subcaption}
\usepackage{float}
\captionsetup{compatibility=false}

\input{numerical_definition.tex}
% report.texと同じディレクトリにnumerical_definition.texを入れておけば上の書き方でもいいはずです

\usepackage[
  dvipdfm,
  bookmarks=true,
  bookmarksnumbered=true,
  colorlinks=true]{hyperref}
\AtBeginDvi{\special{pdf:tounicode EUC-UCS2}}

\pagestyle{labheadings}
\headerleft{全方位画像を用いたシーンの3次元モデルの作成とその活用}   % ヘッダの左側のタイトル
\headerright{2025年7月10日}  % ヘッダの右側のタイトル

\begin{document}

%\twocolumn % 一段組の場合はコメントアウトする

\vspace*{2ex}
\begin{center}
 {\Large \bf 特徴点マッチングの改良と実験の準備}\\ % タイトル
 \vspace*{5mm}
 {\large M2 田川幸汰}% 発表者名
\end{center}

%\vspace{0.5cm} % 一段組の場合はコメントアウトした方が体裁がよいx
%] % 一段組の場合はコメントアウトする

%新しく作成したコマンド
% \newcommand{\reffig}[1]{\hyperref[#1]{図\ref{#1}}}
% \newcommand{\refeq}[1]{\hyperref[#1]{式(\ref{#1})}}
% \newcommand{\reftab}[1]{\hyperref[#1]{表\ref{#1}}}
% \newcommand{\refsec}[1]{\hyperref[#1]{\ref{#1}章}}
% \newcommand{\refsubsec}[1]{\hyperref[#1]{\ref{#1}節}}

% 数式
%\begin{equation}
%  数式記述  
%  \label{ラベル名}
%\end{equation}

% 図
% \begin{figure}[!ht]
%   \begin{center}
%     \includegraphics[scale=0.5]{figures/画像ファイル名}
%     \caption{キャプション名}
%     \label{ラベル名}
%   \end{center}
% \end{figure}

% リスト
% \begin{enumerate or itemize}
%   \item 
% \end{enumerate or itemize}

\section{前回までの結果と概要}
以下に前回までの特徴点マッチングの結果（図\ref{one}）と実行時間（表\ref{table_one}）、
自己位置推定結果（図\ref{two}）を示す。

\begin{figure}[H]
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width=0.45\textwidth]{figures/ML/input_-90.png}&
      \includegraphics[width=0.45\textwidth]{figures/ML/input_-120.png}\\
      \includegraphics[width=0.45\textwidth]{figures/ML/input_-150.png}&
      \includegraphics[width=0.45\textwidth]{figures/ML/c507_60.png}\\
      \includegraphics[width=0.45\textwidth]{figures/ML/c507_90.png}&
      \includegraphics[width=0.45\textwidth]{figures/ML/c507_120.png}
    \end{tabular}
  \end{center}
  \caption{SuperPoint, SuperGlueによる特徴点マッチング結果}
  \label{one}
\end{figure}

\begin{table}[htbp]
  \centering
  \caption{実行時間}
  \label{table_one}
  \begin{tabular}{c|l|l|l|l}
    & 特徴点の事前検出(秒) & 特徴点検出(秒) & 特徴点マッチング(秒) & 自己位置推定(秒) \\
    SuperPoint,SuperGlue & 389.025 & 0.134 & 5.516 & 0.141 \\
  \end{tabular}
\end{table}

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/result.png}
    \caption{自己位置推定結果}
    \label{two}
  \end{center}
\end{figure}

機械学習ベースの特徴点マッチング手法であるSuperPoint~\cite{detone2018superpoint}およびSuperGlue~\cite{sarlin2020superglue}に対して、
現在位置に基づく特徴点マッチングなどの修正を行うことで、マッチング精度および実行時間を改善した。
さらに、自己位置推定の結果として一部のカメラ位置が壁の裏側に推定される問題が発生していたが、
これについても幾何的な制約を導入することで、カメラ位置が正しく壁の表側に推定されるように改善した。

\section{現在位置に基づく特徴点マッチング}
表\ref{table_one}より、処理全体における実行時間のボトルネックは特徴点マッチング処理であることが明らかとなった。
従来は、入力された画像に対して毎回すべてのテクスチャ画像と特徴点マッチングを実施していて、処理時間が大幅に増加していた。
この問題を解消するために、以下の2点の改善を行った。

\begin{enumerate}
  \item \textbf{初回のみ全テクスチャとのマッチングを実施} \\
  最初の1枚の入力画像に対しては、従来通り全テクスチャ画像と特徴点マッチングを実施
  
  \item \textbf{2回目以降は近傍テクスチャに限定} \\
  2回目以降の入力画像については、1つ前の画像から得られた自己位置推定結果（現在位置）を用いて、
  位置的に近いテクスチャ画像を5件選定し、それらとのみ特徴点マッチングを実施
\end{enumerate}

また、この処理変更に伴い、各テクスチャ画像に対応する重心位置をあらかじめファイルに記述・保存する仕組みを追加した。

\subsection{特徴点マッチングの修正}
精度と実行時間の改良のための主な修正項目を以下に示す。

\begin{enumerate}
  \item \textbf{特徴点検出およびマッチングのパラメータ調整} \\
  SuperPointにおける最大検出数（max\_keypoints）を1500から500に削減し、処理時間を短縮
  加えて、SuperGlueにおけるSinkhorn反復回数（sinkhorn\_iterations）を10から15に増やし、マッチングの精度を確保
  また、マッチングのしきい値（match\_threshold）を0.4から0.3に変更し、より信頼性の高いマッチのみを採用

  \item \textbf{距離スコアに基づくマッチ候補の絞り込み} \\
  SuperGlueによって得られたマッチのうち、スコアが高い上位100件のみを選択し、RANSACによる外れ値除去処理に入力するよう変更
  RANSAC処理における計算量を削減

  \item \textbf{不要処理の削減と前処理の導入} \\
  マッチング結果の画像描画は、最良マッチが更新された場合に限って実施するよう変更
  また、特徴点の正規化を事前に行うことで、マッチングモデルへの入力サイズを削減し、GPUメモリの使用効率と処理速度を向上
\end{enumerate}


\section{幾何的な制約による自己位置推定結果の修正}
一部の入力画像において、カメラ位置が壁の裏側に推定される問題が発生していた。
この原因を調査するため、12個の初期値から求めた自己位置推定結果のうち、目的関数のスコアが高い上位3件の解を分析した（図\ref{three}）。
その結果、上位2件はいずれもスコアが同一であり、カメラ位置は壁の裏側に投影され、
さらにカメラ姿勢が天地反転していることが確認された。

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/1.png}
    \caption{カメラ位置}
    \label{three}
  \end{center}
\end{figure}

カメラ位置が壁の裏側に誤って推定される問題に対処するため、カメラ座標系の$Y$軸が常に下向きであるという幾何学的制約を導入した。
具体的には、カメラ姿勢行列$R$の要素$R[1,2]$が負（$R[1,2] < 0$）であることを条件とし、
これはカメラの$Y$軸が世界座標系の$Z$軸負方向と一致していることを意味する。
この判定処理を加えることで、壁の裏側と判定された解を候補から除外することが可能となった。


\subsection{特徴点検出およびマッチング結果}
特徴点検出およびマッチングはSuperPoint、SuperGlueの公式実装(\cite{superglue_github})を参考にした。
実行環境と、SuperPoint及びSuperGlueのパラメータを表\ref{table_two}に示す。
\begin{table}[htbp]
  \centering
  \caption{実行環境及びハイパーパラメータ}
  \label{table_two}
  \begin{tabular}{c|l}
    GPU & NVIDIA GeForce RTX 4060 Ti, NVIDIA driver version=560.94, CUDA version=12.6 \\
    SuperPoint & nms\_radius=4, keypoint\_threshold=0.005, max\_keypoints=500 \\
    SuperGlue & weights='outdoor', sinkhorn\_iterations=15, match\_threshold=0.3 \\
  \end{tabular}
\end{table}

実行結果を図\ref{five}に示す。
\begin{figure}[H]
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width=0.45\textwidth]{figures/ML/input_-90.png}&
      \includegraphics[width=0.45\textwidth]{figures/ML/input_-120.png}\\
      \includegraphics[width=0.45\textwidth]{figures/ML/input_-150.png}&
      \includegraphics[width=0.45\textwidth]{figures/ML/c507_60.png}\\
      \includegraphics[width=0.45\textwidth]{figures/ML/c507_90.png}&
      \includegraphics[width=0.45\textwidth]{figures/ML/c507_120.png}
    \end{tabular}
  \end{center}
  \caption{SuperPoint, SuperGlueによるマッチング結果}
  \label{five}
\end{figure}

SuperPointおよびSuperGlueを用いた場合には、従来の手法とは異なり、テクスチャが豊富な画像において高精度なマッチングが行えるだけでなく、
テクスチャが乏しい画像においても正しいデータベース画像を選択し、特徴点マッチングを安定して行うことができた。
これは、SuperPointが画像全体からより意味的な特徴点を抽出し、SuperGlueが空間的な関係性を考慮したマッチングを行うためであり、
従来の手法（SIFTやAKAZE）に比べて、ロバスト性と適応性の両面で優れていることを示している。
ただし、類似したテクスチャを有する別の画像と誤ってマッチングしてしまう例が一部で見られた。
これは、画像間の構造的類似性や局所的なパターンが極めて近い場合に、SuperPointおよびSuperGlueの判断が困難になるためであると考えられる。


また、特徴点マッチングの結果を用いた自己位置推定の結果を図\ref{six}に示す。
回転行列の初期値として12通りの視点を与え、それぞれについて目的関数を評価し、
最も誤差（目的関数値）が小さい解を大域最適解として採用している。

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/1.png}
    \caption{カメラ位置}
    \label{eight}
  \end{center}
\end{figure}

そのため、カメラ座標系の$Y$軸が常に下向きであるという前提に基づき、
世界座標系における$Z$軸の負方向に一致するかどうか（$R[1,2]<0$）を用いて、「壁の手前か裏側か」を判定する処理を追加した。
この判定により、壁の裏側に位置する解を大域最適解の候補から除外することができた（図\ref{nine}）。

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/result2.png}
    \caption{自己位置推定結果の改良版}
    \label{nine}
  \end{center}
\end{figure}

\section{実行時間の比較}
自己位置推定機能を応用したリアルタイムアプリケーションを開発する際には、特徴点の抽出およびマッチング処理を高速に行う必要がある。
そのため、各手法の実行時間を比較し、リアルタイム性の観点からの評価を行った。比較結果を表\ref{table_three}に示す。

\begin{table}[htbp]
  \centering
  \caption{実行時間(入力画像1の結果)}
  \label{table_three}
  \begin{tabular}{c|l|l|l|l}
    & 特徴点の事前検出(秒) & 特徴点検出(秒) & 特徴点マッチング(秒) & 自己位置推定(秒) \\
    SIFT & 18.312 & 0.248 & 3.151 & 0.166 \\
    AKAZE & 18.213 & 0.182 & 1.939 & 0.123 \\
    SuperPoint,SuperGlue & 389.025 & 0.134 & 5.516 & 0.141 \\
  \end{tabular}
\end{table}

結果より、特徴点の事前検出（テクスチャ画像に対する特徴点検出）に関しては、
SuperPointおよびSuperGlueを用いた手法において実行時間が他の手法と比べて極端に長くなった。
ただし、この処理は各テクスチャ画像に対して一度だけ実行すれば十分であり、
あらかじめ検出した特徴点をファイルとして保存しておくことで、再利用が可能であるため、実行時間の増加は大きな問題とはならない。

また、3手法すべてにおいて特徴点マッチングに比較的長い時間を要しているが、
これは入力画像とすべてのデータベース画像（テクスチャ画像）との間で総当たりのマッチングを行っているためである。
現実の利用においては、自己位置推定結果をもとに、周辺に存在する一部のテクスチャ画像とのみマッチングを行うことが可能であり、
このような工夫により実行時間は大幅に短縮できると考えられる。


\section{今後の計画}
今後の研究計画を以下に示す。
\begin{enumerate}
  \item 7月：C棟5階全範囲の3次元モデル作成（テクスチャ付加）
  \item 8月：線特徴を用いた自己位置推定結果の補助
  \item 8月以降：自己位置推定機能を応用した実用的なアプリケーションのデモ開発
  \begin{itemize}
    \item 生成された3次元モデルと自己位置推定を組み合わせ、目的地までのルートを提示するナビゲーションシステムを構築
  \end{itemize}
\end{enumerate}

\bibliographystyle{ieeetr}  % または plain, unsrt, apalike などスタイルに応じて選択
\bibliography{reference}   % reference.bib というファイル名を使っている場合

\end{document}
